{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d003632",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyarrow in c:\\programdata\\anaconda3\\lib\\site-packages (11.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyarrow) (1.23.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fastparquet in c:\\programdata\\anaconda3\\lib\\site-packages (2023.2.0)\n",
      "Requirement already satisfied: pandas>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from fastparquet) (1.5.3)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from fastparquet) (22.0)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from fastparquet) (1.23.5)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from fastparquet) (2022.11.0)\n",
      "Requirement already satisfied: cramjam>=2.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from fastparquet) (2.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.5.0->fastparquet) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow\n",
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9293c98f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/2023 05:55:44\n",
      "Входной датасет \n",
      "\n",
      " - размерность набора данных -  (12292588, 6)\n",
      "\n",
      "\n",
      "Входной датасет после удаления пустых строк, дубликатов и проверки на информативность \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12292588 entries, 0 to 12292587\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Dtype \n",
      "---  ------           ----- \n",
      " 0   event_date       object\n",
      " 1   event_timestamp  int64 \n",
      " 2   vacancy_id_      int64 \n",
      " 3   cookie_id        object\n",
      " 4   user_id          object\n",
      " 5   event_type       object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 562.7+ MB\n",
      "\n",
      "\n",
      "25/04/2023 05:56:02\n",
      "one hot encoding is in progress..\n",
      "25/04/2023 05:56:02\n",
      "grouping by user is in progress..\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import multiprocessing\n",
    "from multiprocessing import Lock, Process, Queue, current_process\n",
    "import time\n",
    "import queue # imported for using queue.Empty exception\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "\n",
    "# pof.init_notebook_mode()\n",
    "# pio.renderers.default = \"png\"\n",
    "\n",
    "\n",
    "def set_up_printing():\n",
    "    \"\"\"Sets up display parameters\"\"\"\n",
    "\n",
    "    max_dimensions_display_limit = 1000\n",
    "    pd.set_option('display.max_columns', max_dimensions_display_limit)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "    pd.set_option('display.max_rows', max_dimensions_display_limit)\n",
    "    pd.set_option('display.width', None)\n",
    "\n",
    "    pd.set_option('display.max_info_columns', max_dimensions_display_limit)\n",
    "    pd.set_option('display.max_info_rows', max_dimensions_display_limit)\n",
    "\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def get_low_informative(dataframe, limit):\n",
    "    \"\"\"Gets low informative columns\"\"\"\n",
    "    low_information_cols = []\n",
    "\n",
    "    for col in dataframe.columns:\n",
    "        # наибольшая относительная частота значения признака\n",
    "        top_freq = dataframe[col].value_counts(normalize=True).max()\n",
    "        # доля уникальных значений признака\n",
    "        nunique_ratio = dataframe[col].nunique() / dataframe[col].count()\n",
    "        # сравниваем с пороговым значением заданным экспертно\n",
    "        if top_freq > limit:\n",
    "            print(f'{col}: {round(top_freq * 100, 2)}% одинаковых значений')\n",
    "            low_information_cols.append(col)\n",
    "        # сравниваем долю уникальных значений с порогом\n",
    "        if nunique_ratio > limit:\n",
    "            print(f'{col}: {round(nunique_ratio * 100, 2)}% уникальных значений')\n",
    "            low_information_cols.append(col)\n",
    "\n",
    "    return low_information_cols\n",
    "\n",
    "\n",
    "def info_aux_and_nans(dataframe, message):\n",
    "    \"\"\"Prints dataframe info and shows nan stats\"\"\"\n",
    "    print(message, '\\n')\n",
    "    dataframe.info()\n",
    "    dataframe.isnull().sum()\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "def shape(dataframe, message):\n",
    "    \"\"\"Prints dataframe shape\"\"\"\n",
    "    print(message, '\\n')\n",
    "    print(' - размерность набора данных - ', dataframe.shape)\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "def info_aux_and_head(dataframe, message):\n",
    "    \"\"\"Prints dataframe info plas prints first 3 lines\"\"\"\n",
    "    print(message, '\\n')\n",
    "    dataframe.info()\n",
    "    print(dataframe.head(3))\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "def get_vector(x):\n",
    "    print('debug: summing..')\n",
    "    x_sum = x.sum()\n",
    "    x_sum['user_id'] = x.iloc[0, 0]\n",
    "    return x_sum\n",
    "\n",
    "\n",
    "def do_job(tasks_to_accomplish, tasks_that_are_done):\n",
    "    while True:\n",
    "        try:\n",
    "            task = tasks_to_accomplish.get_nowait()\n",
    "        except queue.Empty:\n",
    "\n",
    "            break\n",
    "        else:\n",
    "            vector = get_vector(task)\n",
    "            print(vector)\n",
    "            tasks_that_are_done.put(vector)\n",
    "            time.sleep(.5)\n",
    "    return True\n",
    "\n",
    "\n",
    "# настроим отображение при выводе в консоль\n",
    "set_up_printing()\n",
    "\n",
    "print(datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "\n",
    "# считаем данные и одновременно удалим пустые строки\n",
    "dataframe = pd.read_parquet(r'C:\\kM\\mml\\dataton_2023\\train_mfti.parquet')\n",
    "shape(dataframe, 'Входной датасет')\n",
    "get_low_informative(dataframe, 0.99)\n",
    "info_aux_and_nans(dataframe,\n",
    "                  'Входной датасет после удаления пустых строк, дубликатов и проверки на информативность')\n",
    "\n",
    "dataframe = dataframe.head(1000).drop_duplicates()\n",
    "\n",
    "# в случае продакшен проекта скорее всего стоило бы рассчитать на всех событиях, предварительно\n",
    "# отбирая данные на основе требований заказчика в первую очередь, а также разведочного анализа\n",
    "# в данном случае отбросим все preview значения, что позволит несколько уменьшить размерность\n",
    "dataframe = dataframe[dataframe['event_type'].isin(\n",
    "    ['show_vacancy', 'click_contacts', 'click_favorite', 'click_phone', 'click_response'])]\n",
    "dataframe['user_id'] = dataframe['user_id'].fillna(dataframe['cookie_id'])\n",
    "dataframe['activity'] = dataframe['vacancy_id_'].astype(str) + '_' + dataframe['event_type'].astype(str)\n",
    "dataframe = dataframe[['user_id', 'activity']]\n",
    "\n",
    "print(datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "print('one hot encoding is in progress..')\n",
    "\n",
    "dataframe = pd.get_dummies(dataframe, columns=['activity'], sparse=True)\n",
    "\n",
    "print(datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "print('grouping by user is in progress..')\n",
    "\n",
    "grouped_by_user = dataframe.groupby(['user_id'], sort=False, group_keys=False)[\n",
    "    ['user_id'] + dataframe.columns.tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddf429de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/2023 05:56:45\n",
      "creation of dataframes from groups is in progress..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "print('creation of dataframes from groups is in progress..')\n",
    "\n",
    "tasks_to_accomplish = Queue()\n",
    "i = 0\n",
    "for separated_gdf in (grouped_by_user.get_group(x) for x in grouped_by_user.groups):\n",
    "    i = i + 1\n",
    "    if i % 1000 == 0:\n",
    "        print('Separated one more 100 of gdf: ' + str(i))\n",
    "    tasks_to_accomplish.put(separated_gdf)\n",
    "\n",
    "tasks_to_accomplish.qsize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "451124cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/2023 05:56:59\n",
      "multiprocessing user vectorization is in progress..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "print('multiprocessing user vectorization is in progress..')\n",
    "\n",
    "number_of_processes = 64\n",
    "tasks_that_are_done = Queue()\n",
    "processes = []\n",
    "\n",
    "for w in range(number_of_processes):\n",
    "    p = Process(target=do_job, args=(tasks_to_accomplish, tasks_that_are_done))\n",
    "    processes.append(p)\n",
    "    p.start()\n",
    "\n",
    "for p in processes:\n",
    "    p.join()\n",
    "    \n",
    "tasks_that_are_done.qsize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a09b9ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/2023 05:47:48\n",
      "writing to disc is in progress..\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "print('writing to disc is in progress..')\n",
    "\n",
    "user_vectors_list = list()\n",
    "while not tasks_that_are_done.empty():\n",
    "    print(datetime.datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "    user_vectors_list.append(tasks_that_are_done.get())\n",
    "\n",
    "user_vectors = pd.DataFrame(user_vectors_list)\n",
    "user_vectors.to_csv(r'C:\\Users\\M_N_K\\OneDrive\\Desktop\\kosta_dataton_2023\\user_vectors.to_csv')\n",
    "\n",
    "user_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ee7e68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
